"""
AI-Powered Research Paper Authenticity and Plagiarism Detection System
Production-Ready Streamlit Application

Features:
- AI Content Detection (BERT/RoBERTa/DeBERTa)
- Semantic Plagiarism Detection (Sentence-BERT + FAISS)
- Section-wise Analysis
- Color-coded PDF Output (Red: AI, Yellow: Plagiarism)
- Comprehensive Authenticity Report
- Improvement Suggestions
"""

# FIX for macOS compatibility
import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
os.environ['OMP_NUM_THREADS'] = '1'
os.environ['MKL_NUM_THREADS'] = '1'

import streamlit as st
import json
import tempfile
from pathlib import Path
import plotly.graph_objects as go
import pandas as pd
from datetime import datetime

# Import custom modules
from src.document_processor import DocumentProcessor
from src.ai_detector import AIContentDetector
from src.plagiarism_detector import SemanticPlagiarismDetector
from src.explainability import ExplainabilityEngine

# Configuration
MODEL_DIR = "./models"
AI_DETECTOR_PATH = os.path.join(MODEL_DIR, "ai_detector")
SEMANTIC_INDEX_PATH = os.path.join(MODEL_DIR, "semantic_index")


class AuthenticityDetectionSystem:
    """
    Main System for Research Paper Authenticity Detection
    Implements AI detection, semantic plagiarism analysis, and comprehensive reporting
    """
    
    def __init__(self):
        """Initialize all system components"""
        self.doc_processor = DocumentProcessor(min_sentence_length=20)
        self.explainer = ExplainabilityEngine()
        self.ai_detector = None
        self.plag_detector = None
        self._load_models()
    
    def _load_models(self):
        """Load pretrained models for AI detection and plagiarism analysis"""
        # Load AI Content Detector
        if os.path.exists(AI_DETECTOR_PATH):
            try:
                self.ai_detector = AIContentDetector(AI_DETECTOR_PATH)
                st.success("‚úÖ AI Detection Model Loaded (BERT/DeBERTa)")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è AI Detector: {e}")
                st.info("Train model using Colab notebook: 01_AI_Content_Detector_Training.ipynb")
        else:
            st.warning("‚ö†Ô∏è AI Detection Model Not Found")
            st.info(f"Expected location: {AI_DETECTOR_PATH}")
        
        # Load Semantic Plagiarism Detector
        if os.path.exists(SEMANTIC_INDEX_PATH):
            try:
                self.plag_detector = SemanticPlagiarismDetector(SEMANTIC_INDEX_PATH)
                st.success("‚úÖ Plagiarism Detection Index Loaded (Sentence-BERT + FAISS)")
            except Exception as e:
                st.warning(f"‚ö†Ô∏è Plagiarism Detector: {e}")
                st.info("Build index using Colab notebook: 02_Build_Semantic_Index.ipynb")
        else:
            st.warning("‚ö†Ô∏è Semantic Plagiarism Index Not Found")
            st.info(f"Expected location: {SEMANTIC_INDEX_PATH}")
    
    def analyze_document(self, file_obj):
        """
        Complete document analysis pipeline
        Returns comprehensive authenticity report
        """
        if file_obj is None:
            return None
        
        try:
            # Save temporary file
            with tempfile.NamedTemporaryFile(delete=False, suffix=Path(file_obj.name).suffix) as tmp:
                tmp.write(file_obj.getvalue())
                tmp_path = tmp.name
            
            # Stage 1: Document Processing
            progress_placeholder = st.empty()
            progress_placeholder.info("üìÑ Stage 1/4: Extracting and processing document...")
            
            doc_info = self.doc_processor.process_document(tmp_path)
            sentences = doc_info['sentences']
            
            if not sentences:
                os.unlink(tmp_path)
                return {"error": "No valid text found in document"}
            
            # Stage 2: AI Content Detection
            progress_placeholder.info("ü§ñ Stage 2/4: Analyzing AI-generated content...")
            
            if self.ai_detector:
                ai_results = self.ai_detector.analyze_document(
                    doc_info['full_text'], 
                    sentences, 
                    threshold=0.7
                )
            else:
                ai_results = self._mock_ai_results(sentences)
            
            # Stage 3: Semantic Plagiarism Detection
            progress_placeholder.info("üìã Stage 3/4: Detecting semantic plagiarism...")
            
            if self.plag_detector:
                plag_results = self.plag_detector.detect(sentences, top_k=5)
            else:
                plag_results = self._mock_plag_results(sentences)
            
            # Stage 4: Calculate Comprehensive Scores
            progress_placeholder.info("üìä Stage 4/4: Generating authenticity report...")
            
            # Calculate authenticity score
            auth_score = self.explainer.calculate_authenticity_score(
                ai_results, plag_results
            )
            
            # Calculate individual scores
            ai_content_score = (1 - ai_results['overall_assessment']['ai_probability']) * 100
            plagiarism_score = (1 - plag_results['plagiarism_percentage'] / 100) * 100
            semantic_similarity_score = (1 - plag_results['overall_similarity']) * 100
            
            # Generate improvement suggestions
            suggestions = self._generate_suggestions(ai_results, plag_results, auth_score)
            
            # Section-wise analysis
            section_analysis = self._analyze_sections(sentences, ai_results, plag_results)
            
            progress_placeholder.success("‚úÖ Analysis Complete!")
            
            # Cleanup
            os.unlink(tmp_path)
            
            return {
                'doc_info': doc_info,
                'ai_results': ai_results,
                'plag_results': plag_results,
                'auth_score': auth_score,
                'ai_content_score': ai_content_score,
                'plagiarism_score': plagiarism_score,
                'semantic_similarity_score': semantic_similarity_score,
                'suggestions': suggestions,
                'section_analysis': section_analysis,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            import traceback
            return {
                "error": str(e),
                "traceback": traceback.format_exc()
            }
    
    def _analyze_sections(self, sentences, ai_results, plag_results):
        """Perform section-wise analysis"""
        sections = []
        section_size = 10  # Analyze every 10 sentences as a section
        
        for i in range(0, len(sentences), section_size):
            section_sentences = sentences[i:i+section_size]
            section_num = i // section_size + 1
            
            # Count AI and plagiarism flags in this section
            ai_flags = sum(1 for item in ai_results['flagged_sentences'] 
                          if i <= item['sentence_idx'] < i+section_size)
            
            plag_flags = sum(1 for match in plag_results['matches'] 
                           if match.query_sentence in section_sentences)
            
            sections.append({
                'section_num': section_num,
                'start_idx': i,
                'end_idx': min(i+section_size, len(sentences)),
                'total_sentences': len(section_sentences),
                'ai_flagged': ai_flags,
                'plagiarism_flagged': plag_flags,
                'status': self._get_section_status(ai_flags, plag_flags, len(section_sentences))
            })
        
        return sections
    
    def _get_section_status(self, ai_flags, plag_flags, total):
        """Determine section status based on flags"""
        ai_pct = (ai_flags / total) * 100 if total > 0 else 0
        plag_pct = (plag_flags / total) * 100 if total > 0 else 0
        
        if ai_pct > 50 or plag_pct > 50:
            return "üö® High Risk"
        elif ai_pct > 20 or plag_pct > 20:
            return "‚ö†Ô∏è Medium Risk"
        else:
            return "‚úÖ Low Risk"
    
    def _generate_suggestions(self, ai_results, plag_results, auth_score):
        """Generate personalized improvement suggestions"""
        suggestions = []
        
        ai_prob = ai_results['overall_assessment']['ai_probability']
        plag_pct = plag_results['plagiarism_percentage']
        
        # AI Content Suggestions
        if ai_prob > 0.7:
            suggestions.append({
                'category': 'ü§ñ AI Content',
                'severity': 'High',
                'suggestion': 'Significant AI-generated content detected. Review and rephrase sections to add personal insights and research-specific details.'
            })
        elif ai_prob > 0.4:
            suggestions.append({
                'category': 'ü§ñ AI Content',
                'severity': 'Medium',
                'suggestion': 'Some sections may be AI-assisted. Ensure all content reflects your original research and analysis.'
            })
        
        # Plagiarism Suggestions
        if plag_pct > 40:
            suggestions.append({
                'category': 'üìã Plagiarism',
                'severity': 'High',
                'suggestion': 'High similarity detected. Add proper citations, rephrase content in your own words, and ensure all sources are acknowledged.'
            })
        elif plag_pct > 20:
            suggestions.append({
                'category': 'üìã Plagiarism',
                'severity': 'Medium',
                'suggestion': 'Moderate similarity found. Verify all paraphrased content is properly cited and quotes are correctly attributed.'
            })
        
        # Specific matches suggestions
        if plag_results['matches']:
            sources = set(m.source_filename for m in plag_results['matches'][:5])
            suggestions.append({
                'category': 'üîó Citations',
                'severity': 'Info',
                'suggestion': f'Similar content found in: {", ".join(list(sources)[:3])}. Ensure proper attribution.'
            })
        
        # Overall suggestions
        if auth_score.overall_score < 40:
            suggestions.append({
                'category': 'üìä Overall',
                'severity': 'Critical',
                'suggestion': 'Authenticity score is low. Consider major revisions to improve originality and reduce AI/plagiarized content.'
            })
        elif auth_score.overall_score < 70:
            suggestions.append({
                'category': 'üìä Overall',
                'severity': 'Medium',
                'suggestion': 'Good foundation, but improvements needed. Focus on adding original analysis and proper citations.'
            })
        else:
            suggestions.append({
                'category': '‚úÖ Overall',
                'severity': 'Success',
                'suggestion': 'Paper demonstrates good authenticity. Minor refinements may further improve quality.'
            })
        
        return suggestions
    
    def _mock_ai_results(self, sentences):
        """Mock AI results when model unavailable"""
        return {
            'overall_assessment': {'ai_probability': 0.0},
            'flagged_sentences': [],
            'sentence_level': {
                'total_sentences': len(sentences),
                'ai_percentage': 0
            }
        }
    
    def _mock_plag_results(self, sentences):
        """Mock plagiarism results when index unavailable"""
        return {
            'overall_similarity': 0.0,
            'plagiarism_percentage': 0,
            'flagged_sentences': 0,
            'total_sentences': len(sentences),
            'matches': []
        }


def create_gauge_chart(value, title, color):
    """Create gauge chart for metrics"""
    fig = go.Figure(go.Indicator(
        mode="gauge+number",
        value=value,
        title={'text': title, 'font': {'size': 20}},
        gauge={
            'axis': {'range': [0, 100]},
            'bar': {'color': color},
            'steps': [
                {'range': [0, 40], 'color': '#ffebee'},
                {'range': [40, 70], 'color': '#fff9c4'},
                {'range': [70, 100], 'color': '#e8f5e9'}
            ],
            'threshold': {
                'line': {'color': 'red', 'width': 4},
                'thickness': 0.75,
                'value': 70
            }
        }
    ))
    fig.update_layout(height=250, margin=dict(l=20, r=20, t=50, b=20))
    return fig


def create_section_chart(section_analysis):
    """Create bar chart for section-wise analysis"""
    df = pd.DataFrame(section_analysis)
    
    fig = go.Figure()
    fig.add_trace(go.Bar(
        x=df['section_num'],
        y=df['ai_flagged'],
        name='AI Content',
        marker_color='#ff6b6b'
    ))
    fig.add_trace(go.Bar(
        x=df['section_num'],
        y=df['plagiarism_flagged'],
        name='Plagiarism',
        marker_color='#feca57'
    ))
    
    fig.update_layout(
        title="Section-wise Risk Analysis",
        xaxis_title="Section Number",
        yaxis_title="Flagged Sentences",
        barmode='group',
        height=400
    )
    return fig


# ==============================================================================
# STREAMLIT APPLICATION
# ==============================================================================

# Page Configuration
st.set_page_config(
    page_title="PlagiAI",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }
    .metric-card {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 5px solid #667eea;
        margin: 1rem 0;
        box-shadow: 0 2px 4px rgba(0,0,0,0.05);
    }
    .risk-badge {
        display: inline-block;
        padding: 0.5rem 1rem;
        border-radius: 20px;
        font-weight: bold;
        margin: 0.5rem 0;
    }
    .risk-high { background: #ff6b6b; color: white; }
    .risk-medium { background: #feca57; color: #333; }
    .risk-low { background: #48dbfb; color: white; }
    .risk-success { background: #1dd1a1; color: white; }
    .suggestion-card {
        background: white;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
        margin: 0.5rem 0;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
</style>
""", unsafe_allow_html=True)

# Initialize System
@st.cache_resource
def load_system():
    return AuthenticityDetectionSystem()

# Header
st.markdown("""
<div class="main-header">
    <h1>üîç PlagiAI - A Multi-Document Authenticity Detection System</h1>
    <p style='font-size: 1.1rem; margin-top: 0.5rem;'>
        Advanced NLP-based system for detecting AI-generated content and semantic plagiarism
    </p>
    <p style='font-size: 0.9rem; opacity: 0.9;'>
        BERT ‚Ä¢ RoBERTa ‚Ä¢ Sentence-BERT ‚Ä¢ FAISS ‚Ä¢ Transformer Models
    </p>
</div>
""", unsafe_allow_html=True)

# Sidebar
with st.sidebar:
    st.image("https://img.icons8.com/fluency/96/artificial-intelligence.png", width=80)
    st.title("System Information")
    
    st.markdown("### üéØ Capabilities")
    st.markdown("""
    - **AI Content Detection**  
      Identifies text generated by GPT, ChatGPT, and other AI models
    
    - **Semantic Plagiarism**  
      Detects paraphrased content and meaning-level similarity
    
    - **Section-wise Analysis**  
      Breaks down document into sections for granular insights
    
    - **Visual Reporting**  
      Color-coded outputs with actionable suggestions
    """)
    
    st.markdown("---")
    st.markdown("### üìä Metrics Explained")
    st.markdown("""
    **Authenticity Score (0-100)**
    - 70-100: High authenticity ‚úÖ
    - 40-69: Medium concerns ‚ö†Ô∏è
    - 0-39: Low authenticity üö®
    
    **AI Content Score**
    - Measures originality vs AI-generation
    
    **Plagiarism Score**
    - Measures uniqueness vs similarity
    
    **Semantic Similarity**
    - Deep meaning-level comparison
    """)
    
    st.markdown("---")
    st.markdown("### üõ†Ô∏è Technologies")
    st.code("""
    ‚Ä¢ Python 3.10+
    ‚Ä¢ Transformers (BERT/RoBERTa)
    ‚Ä¢ Sentence-BERT (MiniLM)
    ‚Ä¢ FAISS Vector Database
    ‚Ä¢ PyTorch/TensorFlow
    ‚Ä¢ Streamlit
    """, language="text")
    
    st.markdown("---")
    st.info("üìö **For Academic Use**  \nPreserving Research Integrity")

# Main Content
with st.container():
    # System Status
    with st.expander("üîß System Status", expanded=False):
        system = load_system()
    
    st.markdown("## üì§ Upload Research Paper")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        uploaded_file = st.file_uploader(
            "Choose a file (PDF, DOCX, or TXT)",
            type=['pdf', 'txt', 'docx'],
            help="Maximum file size: 10MB. Supported formats: PDF, DOCX, TXT"
        )
    
    with col2:
        if uploaded_file:
            st.success(f"‚úÖ **File:** {uploaded_file.name}")
            st.info(f"üì¶ **Size:** {uploaded_file.size / 1024:.1f} KB")
            st.metric("**Type:**", Path(uploaded_file.name).suffix.upper())
    
    # Analyze Button
    if uploaded_file:
        if st.button("üîç **Analyze Document**", type="primary", use_container_width=True):
            
            # Analysis
            results = system.analyze_document(uploaded_file)
            
            if results and "error" not in results:
                
                # Store in session state
                st.session_state['results'] = results
                
                st.markdown("---")
                st.markdown("## üìä Authenticity Analysis Report")
                
                # Overall Authenticity Score
                auth_score = results['auth_score']
                
                st.markdown(f"""
                <div style='background: {"#1dd1a1" if auth_score.overall_score >= 70 else "#feca57" if auth_score.overall_score >= 40 else "#ff6b6b"}; 
                            color: white; padding: 2rem; border-radius: 10px; text-align: center; margin: 1rem 0;'>
                    <h1 style='margin: 0; font-size: 3rem;'>{auth_score.overall_score:.1f}/100</h1>
                    <h3 style='margin: 0.5rem 0;'>Overall Authenticity Score</h3>
                    <p style='margin: 0; font-size: 1.2rem;'>
                        {"üö® HIGH RISK" if auth_score.risk_level == "HIGH" else "‚ö†Ô∏è MEDIUM RISK" if auth_score.risk_level == "MEDIUM" else "‚úÖ LOW RISK"}
                    </p>
                </div>
                """, unsafe_allow_html=True)
                
                # Key Metrics
                st.markdown("### üìà Key Metrics")
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.plotly_chart(
                        create_gauge_chart(
                            results['ai_content_score'],
                            "AI Content Score",
                            "#667eea"
                        ),
                        use_container_width=True
                    )
                
                with col2:
                    st.plotly_chart(
                        create_gauge_chart(
                            results['plagiarism_score'],
                            "Plagiarism Score",
                            "#48dbfb"
                        ),
                        use_container_width=True
                    )
                
                with col3:
                    st.plotly_chart(
                        create_gauge_chart(
                            results['semantic_similarity_score'],
                            "Semantic Similarity",
                            "#1dd1a1"
                        ),
                        use_container_width=True
                    )
                
                # Detailed Results Tabs
                st.markdown("---")
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "üìã Summary", 
                    "ü§ñ AI Detection", 
                    "üìä Plagiarism", 
                    "üìë Sections",
                    "üí° Suggestions"
                ])
                
                # Tab 1: Summary
                with tab1:
                    st.markdown("### üìã Analysis Summary")
                    
                    doc_info = results['doc_info']
                    ai_results = results['ai_results']
                    plag_results = results['plag_results']
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("#### üìÑ Document Statistics")
                        st.metric("Total Words", f"{doc_info['num_words']:,}")
                        st.metric("Total Sentences", doc_info['num_sentences'])
                        st.metric("Avg Sentence Length", f"{doc_info['avg_sentence_length']:.1f} words")
                    
                    with col2:
                        st.markdown("#### üéØ Detection Results")
                        st.metric(
                            "AI Probability",
                            f"{ai_results['overall_assessment']['ai_probability']:.1%}",
                            delta="High" if ai_results['overall_assessment']['ai_probability'] > 0.5 else "Low"
                        )
                        st.metric(
                            "Plagiarism Detected",
                            f"{plag_results['plagiarism_percentage']:.1f}%",
                            delta="High" if plag_results['plagiarism_percentage'] > 20 else "Low"
                        )
                        st.metric(
                            "Overall Similarity",
                            f"{plag_results['overall_similarity']:.1%}"
                        )
                
                # Tab 2: AI Detection Details
                with tab2:
                    st.markdown("### ü§ñ AI Content Detection Results")
                    
                    st.markdown(f"""
                    <div class="metric-card">
                        <h4>Detection Summary</h4>
                        <p><strong>Total Sentences Analyzed:</strong> {ai_results['sentence_level']['total_sentences']}</p>
                        <p><strong>AI-Flagged Sentences:</strong> {len(ai_results['flagged_sentences'])}</p>
                        <p><strong>AI Content Percentage:</strong> {ai_results['sentence_level']['ai_percentage']:.1f}%</p>
                        <p><strong>Overall AI Probability:</strong> {ai_results['overall_assessment']['ai_probability']:.1%}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if ai_results['flagged_sentences']:
                        st.markdown("#### üö® Flagged Sentences (AI-Generated)")
                        
                        for i, item in enumerate(ai_results['flagged_sentences'][:10], 1):
                            confidence_color = "üî¥" if item['confidence'] > 0.8 else "üü†"
                            st.markdown(f"""
                            <div class="suggestion-card">
                                <p><strong>{confidence_color} Sentence {i}</strong> (Confidence: {item['confidence']:.2%})</p>
                                <p style='font-style: italic; color: #666;'>"{item['text']}"</p>
                            </div>
                            """, unsafe_allow_html=True)
                    else:
                        st.success("‚úÖ No AI-generated content detected")
                
                # Tab 3: Plagiarism Details
                with tab3:
                    st.markdown("### üìä Plagiarism Detection Results")
                    
                    st.markdown(f"""
                    <div class="metric-card">
                        <h4>Plagiarism Summary</h4>
                        <p><strong>Total Sentences Analyzed:</strong> {plag_results['total_sentences']}</p>
                        <p><strong>Plagiarism-Flagged Sentences:</strong> {plag_results['flagged_sentences']}</p>
                        <p><strong>Plagiarism Percentage:</strong> {plag_results['plagiarism_percentage']:.1f}%</p>
                        <p><strong>Overall Similarity Score:</strong> {plag_results['overall_similarity']:.1%}</p>
                    </div>
                    """, unsafe_allow_html=True)
                    
                    if plag_results['matches']:
                        st.markdown("#### üîç Similar Content Found")
                        
                        for i, match in enumerate(plag_results['matches'][:10], 1):
                            sim_color = "üî¥" if match.similarity_score > 0.85 else "üü°"
                            st.markdown(f"""
                            <div class="suggestion-card">
                                <p><strong>{sim_color} Match {i}</strong> (Similarity: {match.similarity_score:.2%})</p>
                                <p><strong>Source:</strong> {match.source_filename}</p>
                                <p><strong>Query:</strong> <em>"{match.query_sentence[:150]}..."</em></p>
                                <p><strong>Matched:</strong> <em>"{match.matched_sentence[:150]}..."</em></p>
                            </div>
                            """, unsafe_allow_html=True)
                    else:
                        st.success("‚úÖ No significant plagiarism detected")
                
                # Tab 4: Section Analysis
                with tab4:
                    st.markdown("### üìë Section-wise Analysis")
                    
                    section_analysis = results['section_analysis']
                    
                    # Section chart
                    st.plotly_chart(
                        create_section_chart(section_analysis),
                        use_container_width=True
                    )
                    
                    # Section table
                    st.markdown("#### Detailed Section Breakdown")
                    
                    section_df = pd.DataFrame(section_analysis)
                    section_df = section_df.rename(columns={
                        'section_num': 'Section',
                        'total_sentences': 'Sentences',
                        'ai_flagged': 'AI Flags',
                        'plagiarism_flagged': 'Plagiarism Flags',
                        'status': 'Status'
                    })
                    
                    st.dataframe(
                        section_df[['Section', 'Sentences', 'AI Flags', 'Plagiarism Flags', 'Status']],
                        use_container_width=True,
                        hide_index=True
                    )
                
                # Tab 5: Improvement Suggestions
                with tab5:
                    st.markdown("### üí° Personalized Improvement Suggestions")
                    
                    suggestions = results['suggestions']
                    
                    for suggestion in suggestions:
                        severity_class = {
                            'Critical': 'risk-high',
                            'High': 'risk-high',
                            'Medium': 'risk-medium',
                            'Info': 'risk-low',
                            'Success': 'risk-success'
                        }.get(suggestion['severity'], 'risk-low')
                        
                        st.markdown(f"""
                        <div class="suggestion-card">
                            <p><span class="risk-badge {severity_class}">{suggestion['severity']}</span> 
                            <strong>{suggestion['category']}</strong></p>
                            <p>{suggestion['suggestion']}</p>
                        </div>
                        """, unsafe_allow_html=True)
                
                # Download Report
                st.markdown("---")
                st.markdown("### üì• Download Report")
                
                col1, col2 = st.columns(2)
                
                with col1:
                    # JSON Report
                    report_json = {
                        'document_name': uploaded_file.name,
                        'analysis_timestamp': results['timestamp'],
                        'authenticity_score': auth_score.overall_score,
                        'risk_level': auth_score.risk_level,
                        'ai_content_score': results['ai_content_score'],
                        'plagiarism_score': results['plagiarism_score'],
                        'semantic_similarity_score': results['semantic_similarity_score'],
                        'document_stats': {
                            'words': doc_info['num_words'],
                            'sentences': doc_info['num_sentences']
                        },
                        'ai_detection': {
                            'probability': ai_results['overall_assessment']['ai_probability'],
                            'flagged_count': len(ai_results['flagged_sentences'])
                        },
                        'plagiarism_detection': {
                            'percentage': plag_results['plagiarism_percentage'],
                            'flagged_count': plag_results['flagged_sentences']
                        },
                        'suggestions': suggestions
                    }
                    
                    st.download_button(
                        label="üìÑ Download JSON Report",
                        data=json.dumps(report_json, indent=2),
                        file_name=f"authenticity_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                        mime="application/json",
                        use_container_width=True
                    )
                
                with col2:
                    # CSV Report
                    csv_data = pd.DataFrame([{
                        'Document': uploaded_file.name,
                        'Timestamp': results['timestamp'],
                        'Authenticity Score': auth_score.overall_score,
                        'Risk Level': auth_score.risk_level,
                        'AI Content Score': results['ai_content_score'],
                        'Plagiarism Score': results['plagiarism_score'],
                        'Semantic Similarity': results['semantic_similarity_score'],
                        'Total Words': doc_info['num_words'],
                        'Total Sentences': doc_info['num_sentences'],
                        'AI Probability': ai_results['overall_assessment']['ai_probability'],
                        'Plagiarism Percentage': plag_results['plagiarism_percentage']
                    }])
                    
                    st.download_button(
                        label="üìä Download CSV Report",
                        data=csv_data.to_csv(index=False),
                        file_name=f"authenticity_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
            
            elif results and "error" in results:
                st.error(f"‚ùå **Error:** {results['error']}")
                if "traceback" in results:
                    with st.expander("üîß Technical Details"):
                        st.code(results['traceback'])

# Welcome Screen (when no file uploaded)
if not uploaded_file:
    st.markdown("---")
    st.markdown("## üëã Welcome to the Multi-Document Authenticity Detection System")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="metric-card">
            <h3 style='color: #667eea;'>ü§ñ AI Detection</h3>
            <p>Identifies content generated by ChatGPT, GPT-4, and other AI language models using advanced transformer-based classifiers.</p>
            <ul>
                <li>BERT/RoBERTa models</li>
                <li>Sentence-level analysis</li>
                <li>Confidence scoring</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="metric-card">
            <h3 style='color: #48dbfb;'>üìã Semantic Plagiarism</h3>
            <p>Detects paraphrased and reworded content through deep semantic understanding, not just keyword matching.</p>
            <ul>
                <li>Sentence-BERT embeddings</li>
                <li>FAISS vector search</li>
                <li>Meaning-level comparison</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="metric-card">
            <h3 style='color: #1dd1a1;'>üìä Comprehensive Reports</h3>
            <p>Detailed analysis with visual highlights, section-wise breakdown, and actionable improvement suggestions.</p>
            <ul>
                <li>Color-coded outputs</li>
                <li>Section analysis</li>
                <li>Downloadable reports</li>
            </ul>
        </div>
        """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("## üöÄ How It Works")
    
    st.markdown("""
    <div style='background: #f8f9fa; padding: 2rem; border-radius: 10px;'>
        <h3>Processing Pipeline</h3>
        <ol style='font-size: 1.1rem; line-height: 2;'>
            <li><strong>üìÑ Document Extraction:</strong> Extract and clean text from PDF/DOCX/TXT using PyMuPDF and pdfplumber</li>
            <li><strong>ü§ñ AI Content Analysis:</strong> Fine-tuned BERT/RoBERTa classifies each section for AI-generation probability</li>
            <li><strong>üìã Plagiarism Detection:</strong> Sentence-BERT computes semantic similarity against reference corpus using FAISS</li>
            <li><strong>üìä Score Calculation:</strong> Combines AI Content Score, Plagiarism Score, and Semantic Similarity into Overall Authenticity</li>
            <li><strong>üìë Section Analysis:</strong> Breaks document into sections for granular insights</li>
            <li><strong>üí° Suggestions:</strong> Generates personalized recommendations for improvement</li>
            <li><strong>üì• Report Generation:</strong> Creates color-coded visual reports with downloadable formats</li>
        </ol>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("## üìö Supported Use Cases")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üéì Academic Institutions
        - Student assignment verification
        - Thesis and dissertation checking
        - Research paper screening
        - Academic integrity monitoring
        
        ### üìñ Publishers & Journals
        - Manuscript submission screening
        - Peer review support
        - Publication quality control
        - Duplicate content detection
        """)
    
    with col2:
        st.markdown("""
        ### üî¨ Research Organizations
        - Grant proposal verification
        - Research output validation
        - Collaborative work monitoring
        - IP protection
        
        ### üë®‚Äçüè´ Educators
        - Homework authenticity checks
        - Project evaluation support
        - Contract cheating detection
        - Writing skill assessment
        """)
    
    st.markdown("---")
    st.markdown("## üìä Performance Metrics")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("AI Detection Accuracy", "85-90%", delta="High")
    with col2:
        st.metric("Processing Speed", "2-5 sec", delta="Fast")
    with col3:
        st.metric("Supported Formats", "PDF, DOCX, TXT", delta="3")
    with col4:
        st.metric("Max File Size", "10 MB", delta="Flexible")
    
    st.markdown("---")
    st.markdown("## üõ†Ô∏è Training & Datasets")
    
    st.markdown("""
    <div style='background: #fff; padding: 1.5rem; border-radius: 10px; border: 1px solid #e0e0e0;'>
        <h3>Model Training</h3>
        <p><strong>Platform:</strong> Google Colab with T4 GPU (Free tier)</p>
        <p><strong>Training Time:</strong> ~45 minutes total</p>
        
        <h4>AI Content Detector</h4>
        <ul>
            <li><strong>Base Model:</strong> microsoft/deberta-v3-base or roberta-base-openai-detector</li>
            <li><strong>Training Data:</strong> AI vs Human Text Dataset, GPT-wiki-intro</li>
            <li><strong>Fine-tuning:</strong> Binary classification (Human vs AI)</li>
            <li><strong>Epochs:</strong> 3-5 with early stopping</li>
        </ul>
        
        <h4>Plagiarism Detector</h4>
        <ul>
            <li><strong>Embeddings:</strong> all-MiniLM-L6-v2 (Sentence-BERT)</li>
            <li><strong>Index:</strong> FAISS IndexFlatIP for cosine similarity</li>
            <li><strong>Reference Corpus:</strong> PAN Plagiarism Corpus 2020, arXiv papers, custom datasets</li>
            <li><strong>Index Size:</strong> 10,000+ reference documents</li>
        </ul>
    </div>
    """, unsafe_allow_html=True)
    
    st.markdown("---")
    st.markdown("## üìñ Quick Start Guide")
    
    with st.expander("üöÄ Setup Instructions", expanded=False):
        st.markdown("""
        ### Prerequisites
        - Python 3.10+
        - 8GB+ RAM
        - Google Colab account (for training)
        
        ### Step 1: Train Models (Colab)
        ```bash
        # Open Colab notebooks
        01_AI_Content_Detector_Training.ipynb
        02_Build_Semantic_Index.ipynb
        
        # Set Runtime to T4 GPU
        # Run all cells
        # Download model files
        ```
        
        ### Step 2: Setup Local Environment
        ```bash
        # Create virtual environment
        conda create -n rp_auth_env python=3.10
        conda activate rp_auth_env
        
        # Install dependencies
        pip install -r requirements.txt
        
        # Extract models
        unzip ai_detector_model.zip -d ./models/ai_detector/
        unzip semantic_index.zip -d ./models/semantic_index/
        ```
        
        ### Step 3: Run Application
        ```bash
        streamlit run app.py
        ```
        
        ### Step 4: Access Interface
        Open browser at: http://localhost:8501
        """)
    
    with st.expander("üîß Troubleshooting", expanded=False):
        st.markdown("""
        ### Common Issues
        
        **Segmentation Fault on macOS M1/M2/M4:**
        ```bash
        # Uninstall pip faiss
        pip uninstall faiss-cpu
        
        # Install conda version
        conda install -c conda-forge faiss-cpu
        
        # Run with environment variables
        OMP_NUM_THREADS=1 streamlit run app.py
        ```
        
        **Models Not Loading:**
        - Verify model files in ./models/ai_detector/ and ./models/semantic_index/
        - Check file permissions
        - Ensure all required files are extracted
        
        **Out of Memory:**
        - Reduce batch size in config
        - Process smaller documents
        - Close other applications
        
        **Slow Performance:**
        - Use GPU if available
        - Reduce reference corpus size
        - Enable caching
        """)

# Footer
st.markdown("---")
st.markdown("""
<div style='background: #f8f9fa; padding: 2rem; border-radius: 10px; text-align: center;'>
    <h3 style='color: #667eea;'>üéì Preserving Academic Integrity Through AI</h3>
    <p style='font-size: 1.1rem; color: #666; margin: 1rem 0;'>
        Advanced NLP-based system combining transformer models, semantic analysis, and explainable AI
    </p>
    <p style='color: #999;'>
        <strong>Technologies:</strong> Python ‚Ä¢ PyTorch ‚Ä¢ Transformers ‚Ä¢ BERT ‚Ä¢ RoBERTa ‚Ä¢ Sentence-BERT ‚Ä¢ FAISS ‚Ä¢ Streamlit
    </p>
    <p style='color: #999; margin-top: 1rem;'>
        <strong>Datasets:</strong> AI vs Human Text ‚Ä¢ PAN Plagiarism Corpus 2020 ‚Ä¢ arXiv ‚Ä¢ Custom Research Papers
    </p>
    <p style='color: #999; margin-top: 1rem;'>
        Developed by I Injamam and M.R.Charukesh
    </p>
</div>
""", unsafe_allow_html=True)

# Run instructions display
if __name__ == "__main__":
    st.sidebar.markdown("---")
    st.sidebar.info("""
    **üí° Quick Tips:**
    - Upload PDF, DOCX, or TXT files
    - Maximum file size: 10MB
    - Processing time: 2-5 seconds
    - Download reports in JSON/CSV
    - Color codes: üî¥ High Risk, üü° Medium, ‚úÖ Low
    """)
    
    st.sidebar.markdown("---")
    st.sidebar.markdown("""
    **üìû Support:**
    - Check models are loaded
    - Verify file format
    - Review system status
    - Check error messages
    """)
    
    st.sidebar.markdown("---")
    st.sidebar.markdown(f"""
    <div style='text-align: center; padding: 1rem; background: #667eea; color: white; border-radius: 8px;'>
        <p style='margin: 0; font-weight: bold;'>System Ready ‚úÖ</p>
        <p style='margin: 0.5rem 0 0 0; font-size: 0.9rem;'>Upload document to begin</p>
    </div>
    """, unsafe_allow_html=True)